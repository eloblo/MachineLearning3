{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469341c0",
   "metadata": {},
   "source": [
    "# Malicious URL detection\n",
    "Eli Belkind 208250431                                                               \n",
    "Koral Elbaz 318477684                                                                           \n",
    "Itamar Almog 208196600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ab2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import whois\n",
    "from pyquery import PyQuery\n",
    "from requests import get\n",
    "import numpy as np\n",
    "import urllib.request as r\n",
    "import re\n",
    "from xml.dom import minidom\n",
    "import dns.resolver\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e885d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features we extracted from each url\n",
    "features = ['domain_len','hostname_len','https','num_of_dots','num_of_digits','ip_in_url','max_constant','max_vowel',\n",
    "       'special_char_domain','entropy','parameters','fragments','php_url','delimiters','encoded','special_char_url',\n",
    "        'url_depth','sensitive_words_url','A_size','NS_size','MX_size','TXT_size','has_spf','site_is_up','comments','html_size',\n",
    "        'script_size','body_to_script_r','special_char_html','special_char_to_script_r','special_char_to_body_r','num_of_scripts',\n",
    "        'num_of_iframes','num_of_images','num_of_links','num_of_href','num_of_titles','php_html','exe_html','sensitive_words_html',\n",
    "        'malicious_code_count','reach_rank','country_rank','days_since_registration','num_of_registration','days_since_expiration',\n",
    "        'num_of_expiration','days_since_update','num_of_updates','has_ssl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d239dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class that extract data fron the given url\n",
    "class urlF:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.timeout = 3\n",
    "        self.url = url\n",
    "        self.domain = self.getDomain(url)\n",
    "        self.today = datetime.now()\n",
    "        self.dns_res = dns.resolver.Resolver()\n",
    "        self.dns_res.lifetime = self.dns_res.timeout = 1\n",
    "        try:\n",
    "            self.whois = whois.whois(self.domain)\n",
    "        except:\n",
    "            self.whois = None\n",
    "        try:\n",
    "            if url[:7] == \"http://\" or url[:8] == \"https://\":\n",
    "                link = url\n",
    "            else:\n",
    "                link = \"https://\" + url\n",
    "            self.page = get(link, timeout=self.timeout).text\n",
    "            # check for comments before cleaning\n",
    "            if '<!--' in self.page and '-->' in self.page:\n",
    "                self.comments = 1\n",
    "            else:\n",
    "                self.comments = 0\n",
    "            self.page = re.sub(\"(<!--.*?-->)\", \"\", self.page, flags=re.DOTALL)\n",
    "            self.page = self.page.replace('\\n', '')\n",
    "            self.page = self.page.replace('\\t', '')\n",
    "            self.page = self.page.replace('\\r', '')\n",
    "            self.pq = PyQuery(self.page)\n",
    "            self.up = True\n",
    "        except:\n",
    "            self.page = \"\"\n",
    "            self.pq = None\n",
    "            self.up = False\n",
    "            self.comments = 0\n",
    "\n",
    "    def getDomain(self, name):\n",
    "        if name[:7] == \"http://\":\n",
    "            domain = name[7:]\n",
    "        elif name[:8] == \"https://\":\n",
    "            domain = name[8:]\n",
    "        else:\n",
    "            domain = name\n",
    "        if domain[:4] == \"www.\":\n",
    "            domain = domain[4:]\n",
    "        if domain.find('/') != -1:\n",
    "            domain = domain[:domain.find('/')]\n",
    "        return domain\n",
    "\n",
    "    # URL\n",
    "\n",
    "    def num_of_dots(self):\n",
    "        count = 0\n",
    "        for c in self.domain:\n",
    "            if c == '.':\n",
    "                count += 1\n",
    "        return [count]\n",
    "\n",
    "    def digits_in_dom(self):\n",
    "        count = 0\n",
    "        for c in self.domain:\n",
    "            if c.isdigit():\n",
    "                count += 1\n",
    "        return [count]\n",
    "\n",
    "    def ipInUrl(self):\n",
    "        regex = r'[0-9]+(?:\\.[0-9]+){3}'\n",
    "        match = re.search(regex,self.url)\n",
    "        if match:\n",
    "            return [1]\n",
    "        else:\n",
    "            return [0]\n",
    "\n",
    "    def maxc(self):\n",
    "        constant = ['q', 'r', 't', 'p', 's', 'd', 'f', 'g', 'j', 'k', 'l', 'z', 'x', 'c', 'v', 'b', 'n', 'm', 'w', 'h']\n",
    "        maxc = 0\n",
    "        for i in range(len(self.domain) - 1):\n",
    "            count = 0\n",
    "            for j in range(i, len(self.domain)):\n",
    "                if self.domain[j] in constant:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            if count > maxc:\n",
    "                maxc = count\n",
    "        return [maxc]\n",
    "\n",
    "    def maxv(self):\n",
    "        vowls = ['a', 'e', 'o', 'u', 'i']\n",
    "        maxv = 0\n",
    "        for i in range(len(self.domain) - 1):\n",
    "            count = 0\n",
    "            for j in range(i, len(self.domain)):\n",
    "                if self.domain[j] in vowls:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            if count > maxv:\n",
    "                maxv = count\n",
    "        return [maxv]\n",
    "\n",
    "    def getSpecialCharDom(self):\n",
    "        count = 0\n",
    "        for c in self.domain[:self.domain.find('.')]:\n",
    "            if not c.isalpha() and not c.isdigit():\n",
    "                count += 1\n",
    "        return [count]\n",
    "\n",
    "    def getEntropy(self):\n",
    "        string = self.domain.strip()\n",
    "        prob = [float(string.count(c)) / len(string) for c in dict.fromkeys(list(string))]\n",
    "        entropy = -1 * sum([(p * math.log(p,2)) for p in prob])\n",
    "        return [entropy]\n",
    "\n",
    "    def delimeterCount(self):\n",
    "        common_delimiters = [';', '_', '?', '=', '(', ')', ',', '$', '!', '*']\n",
    "        count = 0\n",
    "        for i in common_delimiters:\n",
    "            count += self.url.count(i)\n",
    "        return [count]\n",
    "\n",
    "    def encoded(self):\n",
    "        return [self.url.count('%')]\n",
    "\n",
    "    def specialCharUrl(self):\n",
    "        if self.url[:7] == \"http://\":\n",
    "            name = self.url[7:]\n",
    "        elif self.url[:8] == \"https://\":\n",
    "            name = self.url[8:]\n",
    "        else:\n",
    "            name = self.url\n",
    "        special_char = ['//', '@', '-']\n",
    "        count = 0\n",
    "        for c in special_char:\n",
    "            count += name.count(c)\n",
    "        return [count]\n",
    "\n",
    "    def sensitiveWordsCount(self):\n",
    "        sen_words = ['email', 'account', 'login', 'password','server','free']\n",
    "        count = 0\n",
    "        for i in sen_words:\n",
    "            if i in self.url:\n",
    "                count += 1\n",
    "        return [count]\n",
    "\n",
    "    # DNS\n",
    "\n",
    "    def dnsA(self):\n",
    "        try:\n",
    "            result = self.dns_res.query(self.domain, 'A')\n",
    "            return [len(result)]\n",
    "        except:\n",
    "            return [0]\n",
    "\n",
    "    def dnsNS(self):\n",
    "        try:\n",
    "            result = self.dns_res.query(self.domain, 'NS')\n",
    "            return [len(result)]\n",
    "        except:\n",
    "            return [0]\n",
    "\n",
    "    def dnsMX(self):\n",
    "        try:\n",
    "            result = self.dns_res.query(self.domain, 'MX')\n",
    "            return [len(result)]\n",
    "        except:\n",
    "            return [0]\n",
    "\n",
    "    def dnsTXT(self):\n",
    "        try:\n",
    "            result = self.dns_res.query(self.domain, 'TXT')\n",
    "            length = len(result)\n",
    "            spf = 0\n",
    "            for r in result:\n",
    "                if 'v=spf' in r.to_text():\n",
    "                    spf = 1\n",
    "                    break\n",
    "            return [[length],[spf]]\n",
    "        except:\n",
    "            return [[0],[0]]\n",
    "\n",
    "    # HTML\n",
    "\n",
    "    def getHtmlLength(self):\n",
    "        return [len(self.page)]\n",
    "\n",
    "    def getScriptLength(self):\n",
    "        if self.up:\n",
    "            return [len(self.pq.text('script').text())]\n",
    "        else:\n",
    "            return [0]\n",
    "\n",
    "    def scriptToBodyR(self):\n",
    "        if self.up and not self.getHtmlLength() == 0:\n",
    "            return [self.getScriptLength()[0]/self.getHtmlLength()[0]]\n",
    "        else:\n",
    "            return [0]\n",
    "\n",
    "    def specialCharHtml(self):\n",
    "        if self.up:\n",
    "            count = 0\n",
    "            for c in self.page:\n",
    "                if not c.isalpha() and not c.isdigit():\n",
    "                    count += 1\n",
    "            return [count]\n",
    "        else:\n",
    "            return [0]\n",
    "\n",
    "    def specialCharToBodyR(self):\n",
    "        if self.up and not self.getHtmlLength() == 0:\n",
    "            return [self.specialCharHtml()[0]/self.getHtmlLength()[0]]\n",
    "        else:\n",
    "            return [0]\n",
    "\n",
    "    def specialChatToScript(self):\n",
    "        if self.up and not self.getScriptLength() == 0:\n",
    "            return [self.specialCharHtml()[0]/self.getScriptLength()[0]]\n",
    "        else:\n",
    "            return [0]\n",
    "\n",
    "    def linksCount(self):\n",
    "        links = ['http://', 'https://', 'ftp://', 'gopher://', 'file://', 'mailto://']\n",
    "        count = 0\n",
    "        for l in links:\n",
    "            count += self.page.count(l)\n",
    "        return [count]\n",
    "\n",
    "    def refCount(self):\n",
    "        if self.up:\n",
    "            return [len([i for i in self.pq('a').items()])]\n",
    "        else:\n",
    "            return [0]\n",
    "\n",
    "    def sensitiveTopics(self):\n",
    "        sec_sen_words = ['account', 'bank', 'secure', 'password', 'login', 'signin', 'credit', 'pay', 'click', 'mail'\n",
    "            , 'prize', 'money', 'hack', 'download', 'free', 'now', 'credit', 'buy']\n",
    "        count = 0\n",
    "        for word in sec_sen_words:\n",
    "            if word in self.page:\n",
    "                count += 1\n",
    "        return [count]\n",
    "\n",
    "    def malCode(self):\n",
    "        mal_code_words = ['excec(', 'eval(', 'escape(', 'link(', 'underescape(', 'search(', '.write', 'fromCharCode',\n",
    "                          '.innerHTML','.outerHTML']\n",
    "        count = 0\n",
    "        for word in mal_code_words:\n",
    "            if word in self.page:\n",
    "                count += 1\n",
    "        return [count]\n",
    "\n",
    "    # Context\n",
    "\n",
    "    def siteRank(self):\n",
    "        f = []\n",
    "        path = 'https://data.alexa.com/data?cli=10&dat=snbamz&url=' + self.domain\n",
    "        try:\n",
    "            res = r.urlopen(path, timeout=self.timeout)\n",
    "            dom = minidom.parse(res)\n",
    "            flag = True\n",
    "            for element in dom.getElementsByTagName('REACH'):\n",
    "                if element.hasAttribute('RANK'):\n",
    "                    f.append([element.attributes['RANK'].value])\n",
    "                    flag = False\n",
    "            if flag:\n",
    "                f.append([0])\n",
    "            flag = True\n",
    "            for element in dom.getElementsByTagName('COUNTRY'):\n",
    "                if element.hasAttribute('RANK'):\n",
    "                    f.append([element.attributes['RANK'].value])\n",
    "                    flag = False\n",
    "            if flag:\n",
    "                f.append([0])\n",
    "\n",
    "        except:\n",
    "            f.append([0])\n",
    "            f.append([0])\n",
    "        return f\n",
    "\n",
    "    def daysSinceRegistration(self):\n",
    "        days = 0\n",
    "        size = 0\n",
    "        if self.whois and self.whois.creation_date:\n",
    "            if type(self.whois.creation_date) is datetime:\n",
    "                diff = self.today - self.whois.creation_date\n",
    "                size = 1\n",
    "                days = diff.days\n",
    "            elif type(self.whois.creation_date) is list:\n",
    "                diff = timedelta(days=0)\n",
    "                for reg in self.whois.creation_date:\n",
    "                    if type(reg) is str:\n",
    "                        try:\n",
    "                            tmp = self.today - datetime.strptime(reg, \"%Y-%m-%d %H:%M:%S\")\n",
    "                        except:\n",
    "                            continue\n",
    "                    else:\n",
    "                        tmp = self.today - reg\n",
    "                    if tmp > diff:\n",
    "                        diff = tmp\n",
    "                size = len(self.whois.creation_date)\n",
    "                days = diff.days\n",
    "            elif type(self.whois.creation_date) is str:\n",
    "                try:\n",
    "                    time = datetime.strptime(self.whois.creation_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    diff = self.today - time\n",
    "                    days = diff.days\n",
    "                except:\n",
    "                    days = 0\n",
    "                size = 1\n",
    "        return [[days], [size]]\n",
    "\n",
    "    def daysSinceExpiration(self):\n",
    "        days = 0\n",
    "        size = 0\n",
    "        if self.whois and self.whois.expiration_date:\n",
    "            if type(self.whois.expiration_date) is datetime:\n",
    "                diff = self.whois.expiration_date - self.today\n",
    "                size = 1\n",
    "                days = diff.days\n",
    "            elif type(self.whois.expiration_date) is list:\n",
    "                diff = timedelta(days=0)\n",
    "                for reg in self.whois.expiration_date:\n",
    "                    if type(reg) is str:\n",
    "                        try:\n",
    "                            tmp = datetime.strptime(reg, \"%Y-%m-%d %H:%M:%S\") - self.today\n",
    "                        except:\n",
    "                            continue\n",
    "                    else:\n",
    "                        tmp = reg - self.today\n",
    "                    if tmp > diff:\n",
    "                        diff = tmp\n",
    "                size = len(self.whois.expiration_date)\n",
    "                days = diff.days\n",
    "            elif type(self.whois.expiration_date) is str:\n",
    "                try:\n",
    "                    time = datetime.strptime(self.whois.expiration_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    diff = self.today - time\n",
    "                    days = diff.days\n",
    "                except:\n",
    "                    days = 0\n",
    "                size = 1\n",
    "        return [[days],[size]]\n",
    "\n",
    "    def daysSinceUpdate(self):\n",
    "        days = 0\n",
    "        size = 0\n",
    "        if self.whois and self.whois.updated_date:\n",
    "            if type(self.whois.updated_date) is datetime:\n",
    "                diff = self.today - self.whois.updated_date\n",
    "                size = 1\n",
    "                days = diff.days\n",
    "            elif type(self.whois.updated_date) is list:\n",
    "                diff = timedelta(days=15380)\n",
    "                for reg in self.whois.updated_date:\n",
    "                    if type(reg) is str:\n",
    "                        try:\n",
    "                            tmp = self.today - datetime.strptime(reg, \"%Y-%m-%d %H:%M:%S\")\n",
    "                        except:\n",
    "                            continue\n",
    "                    else:\n",
    "                        tmp = self.today - reg\n",
    "                    if tmp < diff:\n",
    "                        diff = tmp\n",
    "                size = len(self.whois.updated_date)\n",
    "                days = diff.days\n",
    "            elif type(self.whois.updated_date) is str:\n",
    "                try:\n",
    "                    time = datetime.strptime(self.whois.updated_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    diff = self.today - time\n",
    "                    days = diff.days\n",
    "                except:\n",
    "                    days = 0\n",
    "                size = 1\n",
    "        return [[days],[size]]\n",
    "\n",
    "    def hasSSL(self):\n",
    "        try:\n",
    "            ssl.get_server_certificate((self.domain, 443))\n",
    "            return [1]\n",
    "        except:\n",
    "            return [0]\n",
    "\n",
    "    def getFeatures(self):\n",
    "        f = []\n",
    "\n",
    "        # URL\n",
    "\n",
    "        f.append([len(self.domain)])  # hostname length\n",
    "        f.append([len(self.domain[:self.domain.find('.')])])  # domain length\n",
    "\n",
    "        if self.url[:8] == \"https://\":\n",
    "            f.append([1])\n",
    "        else:\n",
    "            f.append([0])\n",
    "\n",
    "        f.append(self.num_of_dots())\n",
    "        f.append(self.digits_in_dom())\n",
    "        f.append(self.ipInUrl())\n",
    "        f.append(self.maxc())\n",
    "        f.append(self.maxv())\n",
    "        f.append(self.getSpecialCharDom())\n",
    "        f.append(self.getEntropy())\n",
    "\n",
    "        # parameters and fragments\n",
    "        f.append([len(self.url.split('&')) - 1])\n",
    "        f.append([len(self.url.split('#')) - 1])\n",
    "\n",
    "        # suspicious files\n",
    "        if '.php' in self.url:\n",
    "            f.append([1])\n",
    "        else:\n",
    "            f.append([0])\n",
    "\n",
    "        f.append(self.delimeterCount())\n",
    "        f.append(self.encoded())\n",
    "        f.append(self.specialCharUrl())\n",
    "        # url depth\n",
    "        f.append([self.url.count('/') - 2 * self.url.count('//')])\n",
    "        f.append(self.sensitiveWordsCount())\n",
    "\n",
    "        # DNS\n",
    "\n",
    "        f.append(self.dnsA())\n",
    "        f.append(self.dnsNS())\n",
    "        f.append(self.dnsMX())\n",
    "        res = self.dnsTXT()\n",
    "        for item in res:\n",
    "            f.append(item)\n",
    "\n",
    "        # HTML\n",
    "\n",
    "        if self.up:\n",
    "            f.append([1])\n",
    "        else:\n",
    "            f.append([0])\n",
    "\n",
    "        f.append([self.comments])\n",
    "        f.append(self.getHtmlLength())\n",
    "        f.append(self.getScriptLength())\n",
    "        f.append(self.scriptToBodyR())\n",
    "        f.append(self.specialCharHtml())\n",
    "        f.append(self.specialCharToBodyR())\n",
    "        f.append(self.specialChatToScript())\n",
    "\n",
    "        # unique titles\n",
    "        f.append([self.page.count('</script>')])\n",
    "        f.append([self.page.count('</iframe>')])\n",
    "        f.append([self.page.count('</image>')])\n",
    "\n",
    "        f.append(self.linksCount())\n",
    "        f.append(self.refCount())\n",
    "\n",
    "        # sum of titles\n",
    "        f.append([self.page.count('</')])\n",
    "\n",
    "        # references to files\n",
    "        if '.php' in self.page:\n",
    "            f.append([1])\n",
    "        else:\n",
    "            f.append([0])\n",
    "\n",
    "        if '.exe' in self.page:\n",
    "            f.append([1])\n",
    "        else:\n",
    "            f.append([0])\n",
    "\n",
    "        f.append(self.sensitiveTopics())\n",
    "        f.append(self.malCode())\n",
    "\n",
    "        # Context\n",
    "\n",
    "        rank = self.siteRank()\n",
    "        for i in rank:\n",
    "            f.append(i)\n",
    "\n",
    "        reg = self.daysSinceRegistration()\n",
    "        for i in reg:\n",
    "            f.append(i)\n",
    "\n",
    "        exp = self.daysSinceExpiration()\n",
    "        for i in exp:\n",
    "            f.append(i)\n",
    "\n",
    "        update = self.daysSinceUpdate()\n",
    "        for i in update:\n",
    "            f.append(i)\n",
    "\n",
    "        f.append(self.hasSSL())\n",
    "\n",
    "        return np.array(f).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9181ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the data base we read eaxh url and extract the data, and then save it in a csv file\n",
    "# because we download data from the url and from other tools like alexa and whois, the process can take a while\n",
    "col = features\n",
    "df = pd.read_csv('malicious_phish.csv')\n",
    "url = df['url']\n",
    "urlDF = pd.DataFrame(urlF(url.iloc[0]).getFeatures(), columns=col)\n",
    "for i in url.index:\n",
    "    if i == 0:\n",
    "        continue\n",
    "    tmp = pd.DataFrame(urlF(url.iloc[i]).getFeatures(), index=[i], columns=col)\n",
    "    urlDF = pd.concat([urlDF, tmp], axis=0)\n",
    "df = pd.concat([df, urlDF], axis=1)\n",
    "file = 'C:\\Users\\elobl\\PycharmProjects\\ml_finale\\data\\full_phish.csv'\n",
    "df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b85baf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1571   38]\n",
      " [  71  520]]\n",
      "0.9763828464885022 0.9567600487210719 0.9504545454545454 0.9664718548139034\n"
     ]
    }
   ],
   "source": [
    "# after normalization we put the data in our ML model\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\elobl\\\\PycharmProjects\\\\ml_finale\\\\data\\\\full_phish.csv')\n",
    "label = df['label']\n",
    "df = df.drop(['label','url'], axis=1)\n",
    "\n",
    "x = df\n",
    "y = label\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "\n",
    "model.fit(x_train,y_train)\n",
    "predictions = model.predict(x_test)\n",
    "true_labels = y_test\n",
    "cf_matrix = confusion_matrix(true_labels, predictions)\n",
    "print(cf_matrix)\n",
    "recall = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[0][1])\n",
    "precision = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[1][0])\n",
    "auc = (cf_matrix[0][0] + cf_matrix[1][1]) / (cf_matrix[0][0] + cf_matrix[1][1] + cf_matrix[1][0] + cf_matrix[0][1])\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(recall, precision, auc, f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
